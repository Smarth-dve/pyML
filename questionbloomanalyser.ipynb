{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing the necessary libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom keras.layers import Input, Embedding, LSTM, Dense, Bidirectional, SimpleRNN\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/questionanalyserdataset/QuestionAnalyserDataset.csv\")\ndf = df[['Questions','Blooms Taxonomy']]\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing the words in the Dataframe","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['Questions'])\nsequences = tokenizer.texts_to_sequences(df['Questions'])\nmax_len = max([len(seq) for seq in sequences])\npadded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(df['Blooms Taxonomy'])\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spliting the data into training and testing sets","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\nprint(type(X_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the model architecture\nNote that Here I have used the Bidirectional LSTM model *(As this topic is similar to Sentiment analysis)* ","metadata":{}},{"cell_type":"code","source":"inputs = Input(shape=(max_len,))\nx = Embedding(vocab_size, 128, input_length=max_len)(inputs)\nx = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(x)\noutputs = Dense(50, activation='sigmoid')(x)\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score, acc = model.evaluate(X_test, y_test, verbose=2)\nprint(\"Validation accuracy:\", acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make predictions on new Data","metadata":{}},{"cell_type":"code","source":"new_questions = ['What types of programming languages are vulnerable to buffer overflows?', 'Construct the Binary Search Tree using following data. Show each steps. 32, 45, 12, 11, 13, 92, 78, 66, 17, 70,98, 108. Show its Preorder, Inorder and Postorder traversing sequences.']\nnew_sequences = tokenizer.texts_to_sequences(new_questions)\nnew_padded_sequences = pad_sequences(new_sequences, maxlen=max_len, padding='post')\npredictions = model.predict(new_padded_sequences)\npredicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\nprint(predicted_labels)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementation using Glove","metadata":{}},{"cell_type":"code","source":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\nfrom keras.models import Model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the Bloom's Taxonomy dataset\n#df = pd.read_csv(\"QuestionAnalyserDatasetUpdated.csv\")\n#df = df[['Questions','Blooms Taxonomy']]  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the pre-trained GloVe embeddings\nword_embeddings = {}\nwith open('/kaggle/input/glove6b/glove.6B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        word_embeddings[word] = coefs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize the text and convert it to sequences\ntokenizer = keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(df['Questions'])\nsequences = tokenizer.texts_to_sequences(df['Questions'])\nmax_len = max([len(seq) for seq in sequences])\npadded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an embedding matrix for the pre-trained GloVe embeddings\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = word_embeddings.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the labels to numerical values\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(df['Blooms Taxonomy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model architecture\ninputs = Input(shape=(max_len,))\nx = Embedding(len(tokenizer.word_index) + 1, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)(inputs)\nx = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(x)\noutputs = Dense(80, activation='softmax')(x)\nmodel = Model(inputs=inputs, outputs=outputs)\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score, acc = model.evaluate(X_test, y_test, verbose=2)\nprint(\"Validation accuracy:\", acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on new data\nnew_questions = ['What can be the maximum number of nodes in binary tree with height 4?', 'Write an algorithm to insert a node at beginning in circular linked list.']\nnew_sequences = tokenizer.texts_to_sequences(new_questions)\nnew_padded_sequences = pad_sequences(new_sequences, maxlen=max_len, padding='post')\npredictions = model.predict(new_padded_sequences)\npredicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\nprint(predicted_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Load the data\ndf = pd.read_csv(\"/kaggle/input/questionanalyserdataset/QuestionAnalyserDataset.csv\")\ndf = df[['Questions','Blooms Taxonomy']]\n\n# Tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['Questions'])\nsequences = tokenizer.texts_to_sequences(df['Questions'])\nmax_len = max([len(seq) for seq in sequences])\npadded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\nvocab_size = len(tokenizer.word_index) + 1\n\n# Encode the labels\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(df['Blooms Taxonomy'])\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n\n# Define the model architecture\ninputs = Input(shape=(max_len,))\nx = Embedding(vocab_size, 128, input_length=max_len)(inputs)\nx = Bidirectional(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(x)\nx = Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = Dense(64, activation='relu')(x)\noutputs = Dense(50, activation='softmax')(x)\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Compile the model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32)\n\n# Evaluate the model on the test set\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', test_loss)\nprint('Test accuracy:', test_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold","metadata":{"execution":{"iopub.status.busy":"2023-04-01T17:03:02.402208Z","iopub.execute_input":"2023-04-01T17:03:02.402580Z","iopub.status.idle":"2023-04-01T17:03:02.409114Z","shell.execute_reply.started":"2023-04-01T17:03:02.402548Z","shell.execute_reply":"2023-04-01T17:03:02.408118Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load the data\ndf = pd.read_csv(\"/kaggle/input/questionanalyserdataset/QuestionAnalyserDataset.csv\")\ndf = df[['Questions','Blooms Taxonomy']]","metadata":{"execution":{"iopub.status.busy":"2023-04-01T17:03:05.279387Z","iopub.execute_input":"2023-04-01T17:03:05.279749Z","iopub.status.idle":"2023-04-01T17:03:05.296729Z","shell.execute_reply.started":"2023-04-01T17:03:05.279718Z","shell.execute_reply":"2023-04-01T17:03:05.295778Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['Questions'])\nsequences = tokenizer.texts_to_sequences(df['Questions'])\nmax_len = max([len(seq) for seq in sequences])\npadded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2023-04-01T17:03:09.593540Z","iopub.execute_input":"2023-04-01T17:03:09.593897Z","iopub.status.idle":"2023-04-01T17:03:09.720031Z","shell.execute_reply.started":"2023-04-01T17:03:09.593865Z","shell.execute_reply":"2023-04-01T17:03:09.719053Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Encode the labels\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(df['Blooms Taxonomy'])","metadata":{"execution":{"iopub.status.busy":"2023-04-01T17:03:10.886529Z","iopub.execute_input":"2023-04-01T17:03:10.886892Z","iopub.status.idle":"2023-04-01T17:03:10.893003Z","shell.execute_reply.started":"2023-04-01T17:03:10.886860Z","shell.execute_reply":"2023-04-01T17:03:10.891885Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Define the model architecture\ninputs = Input(shape=(max_len,))\nx = Embedding(vocab_size, 128, input_length=max_len)(inputs)\nx = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = Dense(64, activation='relu')(x)\noutputs = Dense(50, activation='softmax')(x)\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Define k-fold cross validation\nk = 5\nkfold = KFold(n_splits=k, shuffle=True)\n\n# Perform k-fold cross validation\ntest_losses = []\ntest_accuracies = []\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T17:03:13.294666Z","iopub.execute_input":"2023-04-01T17:03:13.295809Z","iopub.status.idle":"2023-04-01T17:03:13.725809Z","shell.execute_reply.started":"2023-04-01T17:03:13.295760Z","shell.execute_reply":"2023-04-01T17:03:13.724870Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for train_idx, test_idx in kfold.split(padded_sequences):\n    X_train, X_test = padded_sequences[train_idx], padded_sequences[test_idx]\n    y_train, y_test = labels[train_idx], labels[test_idx]\n\n    # Compile the model\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Train the model\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64, verbose=0)\n\n    # Evaluate the model on the test set\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    test_losses.append(test_loss)\n    test_accuracies.append(test_acc)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T17:03:49.867984Z","iopub.execute_input":"2023-04-01T17:03:49.868672Z","iopub.status.idle":"2023-04-01T17:11:47.915484Z","shell.execute_reply.started":"2023-04-01T17:03:49.868634Z","shell.execute_reply":"2023-04-01T17:11:47.914112Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3796739070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Evaluate the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Print the average test loss and accuracy across all folds\nprint('Average test loss:', np.mean(test_losses))\nprint('Average test accuracy:', np.mean(test_accuracies))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bidirectional RNN using Cross Validation (k=5)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom keras.layers import Input, Embedding, LSTM, Dense, Bidirectional, SimpleRNN\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold\n\n# Load the data\ndf = pd.read_csv(\"/kaggle/input/questionanalyserdataset/QuestionAnalyserDataset.csv\")\ndf = df[['Questions','Blooms Taxonomy']]\n\n# Tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['Questions'])\nsequences = tokenizer.texts_to_sequences(df['Questions'])\nmax_len = max([len(seq) for seq in sequences])\npadded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\nvocab_size = len(tokenizer.word_index) + 1\n\n# Encode the labels\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(df['Blooms Taxonomy'])\n\n# Define the model architecture\ninputs = Input(shape=(max_len,))\nx = Embedding(vocab_size, 128, input_length=max_len)(inputs)\nx = Bidirectional(LS(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(x)\nx = Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = Dense(64, activation='relu')(x)\noutputs = Dense(50, activation='softmax')(x)\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Define k-fold cross validation\nk = 5\nkfold = KFold(n_splits=k, shuffle=True)\n\n# Perform k-fold cross validation\ntest_losses = []\ntest_accuracies = []\nfor i, (train_idx, test_idx) in enumerate(kfold.split(padded_sequences)):\n    X_train, X_test = padded_sequences[train_idx], padded_sequences[test_idx]\n    y_train, y_test = labels[train_idx], labels[test_idx]\n\n    # Compile the model\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Train the model\n    print('Fold:', i+1)\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64, verbose=1)\n\n    # Evaluate the model on the test set\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    test_losses.append(test_loss)\n    test_accuracies.append(test_acc)\n\n# Print the average test loss and accuracy across all folds\nprint('Average test loss:', np.mean(test_losses))\nprint('Average test accuracy:', np.mean(test_accuracies))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T20:01:01.579842Z","iopub.execute_input":"2023-04-01T20:01:01.580221Z","iopub.status.idle":"2023-04-01T20:08:40.292689Z","shell.execute_reply.started":"2023-04-01T20:01:01.580186Z","shell.execute_reply":"2023-04-01T20:08:40.291313Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 203)]             0         \n_________________________________________________________________\nembedding_3 (Embedding)      (None, 203, 128)          409088    \n_________________________________________________________________\nbidirectional_6 (Bidirection (None, 256)               263168    \n_________________________________________________________________\ndense_6 (Dense)              (None, 50)                12850     \n=================================================================\nTotal params: 685,106\nTrainable params: 685,106\nNon-trainable params: 0\n_________________________________________________________________\nFold: 1\nEpoch 1/50\n36/36 [==============================] - 65s 2s/step - loss: 2.2815 - accuracy: 0.2048 - val_loss: 1.7830 - val_accuracy: 0.1566\nEpoch 2/50\n36/36 [==============================] - 60s 2s/step - loss: 1.6106 - accuracy: 0.3375 - val_loss: 1.4035 - val_accuracy: 0.4377\nEpoch 3/50\n36/36 [==============================] - 60s 2s/step - loss: 1.2118 - accuracy: 0.5263 - val_loss: 1.2007 - val_accuracy: 0.5249\nEpoch 4/50\n36/36 [==============================] - 60s 2s/step - loss: 0.9366 - accuracy: 0.6354 - val_loss: 0.9322 - val_accuracy: 0.6584\nEpoch 5/50\n36/36 [==============================] - 61s 2s/step - loss: 0.7151 - accuracy: 0.7382 - val_loss: 0.7814 - val_accuracy: 0.7313\nEpoch 6/50\n36/36 [==============================] - 60s 2s/step - loss: 0.5573 - accuracy: 0.7890 - val_loss: 0.7108 - val_accuracy: 0.7438\nEpoch 7/50\n36/36 [==============================] - 60s 2s/step - loss: 0.4623 - accuracy: 0.8335 - val_loss: 0.5930 - val_accuracy: 0.7989\nEpoch 8/50\n19/36 [==============>...............] - ETA: 27s - loss: 0.3287 - accuracy: 0.8947","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2263701835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fold:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Evaluate the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Bidirectional LSTM using Cross validation (k = 5)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom keras.layers import Input, Embedding, LSTM, Dense, Bidirectional, SimpleRNN\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold\n\n# Load the data\ndf = pd.read_csv(\"/kaggle/input/questionanalyserdataset/QuestionAnalyserDataset.csv\")\ndf = df[['Questions','Blooms Taxonomy']]\n\n# Tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['Questions'])\nsequences = tokenizer.texts_to_sequences(df['Questions'])\nmax_len = max([len(seq) for seq in sequences])\npadded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\nvocab_size = len(tokenizer.word_index) + 1\n\n# Encode the labels\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(df['Blooms Taxonomy'])\n\ninputs = Input(shape=(max_len,))\nx = Embedding(vocab_size, 128, input_length=max_len)(inputs)\nx = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(x)\noutputs = Dense(50, activation='sigmoid')(x)\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()\n\n# Define k-fold cross validation\nk = 5\nkfold = KFold(n_splits=k, shuffle=True)\n\n# Perform k-fold cross validation\ntest_losses = []\ntest_accuracies = []\nfor i, (train_idx, test_idx) in enumerate(kfold.split(padded_sequences)):\n    X_train, X_test = padded_sequences[train_idx], padded_sequences[test_idx]\n    y_train, y_test = labels[train_idx], labels[test_idx]\n\n    # Compile the model\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Train the model\n    print('Fold:', i+1)\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64, verbose=1)\n\n    # Evaluate the model on the test set\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    test_losses.append(test_loss)\n    test_accuracies.append(test_acc)\n\n# Print the average test loss and accuracy across all folds\nprint('Average test loss:', np.mean(test_losses))\nprint('Average test accuracy:', np.mean(test_accuracies))","metadata":{},"execution_count":null,"outputs":[]}]}